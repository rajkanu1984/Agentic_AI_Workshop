{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf faiss-cpu langchain langchain-google-genai PyPDF2 python-dotenv"
      ],
      "metadata": {
        "id": "-TcDBvL-69pP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ydp-37H7owl",
        "outputId": "25f34c61-0c47-4b00-957a-664c881c10c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "import textwrap\n",
        "import os\n",
        "\n",
        "# Set your Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCXAypufNrgwkghOfBgDbz4Om8TJ8PlUWA\"  # Replace with your actual key"
      ],
      "metadata": {
        "id": "FPNurzx37e0r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdfs(pdf_paths):\n",
        "    \"\"\"Extract text from PDFs with page metadata\"\"\"\n",
        "    documents = []\n",
        "    for path in pdf_paths:\n",
        "        try:\n",
        "            pdf_reader = PdfReader(path)\n",
        "            for page_num, page in enumerate(pdf_reader.pages):\n",
        "                text = page.extract_text()\n",
        "                if text.strip():\n",
        "                    metadata = {\n",
        "                        \"source\": os.path.basename(path),\n",
        "                        \"page\": page_num + 1\n",
        "                    }\n",
        "                    documents.append(Document(page_content=text, metadata=metadata))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {path}: {str(e)}\")\n",
        "    return documents\n",
        "\n",
        "def chunk_documents(documents):\n",
        "    \"\"\"Split documents into manageable chunks\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=300,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "tKZUe02g8GkC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks):\n",
        "    \"\"\"Create FAISS vector store using Gemini embeddings\"\"\"\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    return FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "9A1F34Le8Pea"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_qa_chain(vector_store):\n",
        "    \"\"\"Create retrieval QA chain with Gemini Pro\"\"\"\n",
        "    # Custom prompt for research paper QA\n",
        "    prompt_template = \"\"\"\n",
        "    You are an AI research assistant. Answer the question based only on the following context which comes from AI research papers.\n",
        "    Provide detailed, technical answers and always cite your sources using the document metadata.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer in markdown format with clear section references:\n",
        "    \"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Initialize Gemini Pro\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0.3)\n",
        "\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        chain_type_kwargs={\"prompt\": PROMPT},\n",
        "        return_source_documents=True\n",
        "    )"
      ],
      "metadata": {
        "id": "SfO7zmX-8UdI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Upload PDFs first (use Colab file upload)\n",
        "    pdf_paths = [\n",
        "        \"/content/paper1.pdf\",\n",
        "        \"/content/paper2.pdf\",\n",
        "        \"/content/paper3.pdf\"\n",
        "    ]\n",
        "\n",
        "    print(\"‚è≥ Loading and processing PDFs...\")\n",
        "    raw_docs = load_pdfs(pdf_paths)\n",
        "    chunks = chunk_documents(raw_docs)\n",
        "    print(f\"‚úÖ Processed {len(raw_docs)} documents into {len(chunks)} chunks\")\n",
        "\n",
        "    print(\"üß† Creating vector database...\")\n",
        "    vector_store = create_vector_store(chunks)\n",
        "    print(\"‚úÖ Vector database created\")\n",
        "\n",
        "    qa_chain = setup_qa_chain(vector_store)\n",
        "\n",
        "    # Interactive question answering\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        question = input(\"\\nAsk a question about the research (type 'exit' to quit):\\n\")\n",
        "\n",
        "        if question.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        print(\"\\nüîç Searching and generating answer...\")\n",
        "        result = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "        # Print formatted answer\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üí° ANSWER:\")\n",
        "        print(textwrap.fill(result[\"result\"], width=100))\n",
        "\n",
        "        # Print sources\n",
        "        print(\"\\nüìö SOURCES:\")\n",
        "        unique_sources = set()\n",
        "        for doc in result[\"source_documents\"]:\n",
        "            source_info = f\"{doc.metadata['source']} (page {doc.metadata['page']})\"\n",
        "            if source_info not in unique_sources:\n",
        "                unique_sources.add(source_info)\n",
        "                print(f\"- {source_info}\")\n",
        "\n",
        "        print(\"=\"*50)"
      ],
      "metadata": {
        "id": "aVY2_nxv8mul"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYBATOPj-YuL",
        "outputId": "05f0a373-b0fb-4d2c-d21d-ce93b3d2d9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Loading and processing PDFs...\n",
            "‚úÖ Processed 109 documents into 249 chunks\n",
            "üß† Creating vector database...\n",
            "‚úÖ Vector database created\n",
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the research (type 'exit' to quit):\n",
            "Main components of RAG model and how they interact?\n",
            "\n",
            "üîç Searching and generating answer...\n",
            "\n",
            "==================================================\n",
            "üí° ANSWER:\n",
            "The RAG model consists of a retriever and a generator that work together to produce text. The\n",
            "retriever identifies relevant documents based on the input, and the generator uses these documents\n",
            "to generate the output text. ([2.1 Models])  The interaction between these components is as follows:\n",
            "1.  **Retrieval:** Given an input x, the retriever identifies the top-K documents, denoted as z,\n",
            "based on  p(z|x). ([2.1 Models]) 2.  **Generation:** The generator, conditioned on the input x and\n",
            "the retrieved document z, generates the output sequence y. The probability of generating a token is\n",
            "given by p(y\\_i|x, z, y\\_{1:i-1}), where y\\_{1:i-1} represents the tokens generated before the\n",
            "current token. ([2.1 Models]) 3.  **Marginalization:** The model marginalizes over the latent\n",
            "documents to produce a distribution over generated text. Two approaches are used: RAG-Sequence and\n",
            "RAG-Token. ([2.1 Models])      *   **RAG-Sequence:** Uses the same retrieved document to predict\n",
            "each target token. The probability is calculated as: pRAG-Sequence(y|x) ‚âà Œ£ z‚ààtop-k(p(z|x)) p(z|x)\n",
            "p(y|x, z) = Œ£ z‚ààtop-k(p(z|x)) p(z|x) ‚àèi p(y\\_i|x, z, y\\_{1:i-1}). ([2.1 Models])     *   **RAG-\n",
            "Token:**  Draws a different latent document for each target token and marginalizes accordingly. The\n",
            "probability is calculated as: pRAG-Token(y|x) ‚âà ‚àèi Œ£ z‚ààtop-k(p(z|x)) p(z|x) p(y\\_i|x, z,\n",
            "y\\_{1:i-1}). ([2.1 Models])\n",
            "\n",
            "üìö SOURCES:\n",
            "- paper3.pdf (page 18)\n",
            "- paper3.pdf (page 4)\n",
            "- paper3.pdf (page 3)\n",
            "- paper3.pdf (page 5)\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the research (type 'exit' to quit):\n",
            "what are the two sublayers in each encoder layer of the transformer model\n",
            "\n",
            "üîç Searching and generating answer...\n",
            "\n",
            "==================================================\n",
            "üí° ANSWER:\n",
            "Each encoder layer in the Transformer model consists of two sub-layers, as described in section 3.1\n",
            "of the paper:  1.  **Multi-head self-attention mechanism:** This sub-layer allows the encoder to\n",
            "attend to different parts of the input sequence and capture relationships between them (\"Encoder\",\n",
            "section 3.1). 2.  **Position-wise fully connected feed-forward network:** This sub-layer further\n",
            "processes the output of the self-attention mechanism, applying the same feed-forward network to each\n",
            "position in the sequence independently (\"Encoder\", section 3.1).\n",
            "\n",
            "üìö SOURCES:\n",
            "- paper1.pdf (page 3)\n",
            "- paper1.pdf (page 2)\n",
            "- paper1.pdf (page 10)\n",
            "- paper1.pdf (page 9)\n",
            "- paper1.pdf (page 5)\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the research (type 'exit' to quit):\n",
            "Question 3: Explain how positional encoding is implemented in transformers and why it is necessary\n",
            "\n",
            "üîç Searching and generating answer...\n",
            "\n",
            "==================================================\n",
            "üí° ANSWER:\n",
            "### Positional Encoding in Transformers  The Transformer model architecture uses positional encoding\n",
            "to incorporate information about the position of tokens in a sequence, since it does not use\n",
            "recurrence and therefore has no inherent sense of order. [2]  **Implementation:**  The original\n",
            "Transformer model uses sinusoidal functions for positional encoding. However, learned positional\n",
            "embeddings can also be used, with nearly identical results [2].  **Necessity:**  Positional encoding\n",
            "is necessary because the Transformer architecture eschews recurrence and relies entirely on\n",
            "attention mechanisms [2]. Unlike recurrent neural networks (RNNs), which process sequences\n",
            "sequentially and inherently capture positional information, the Transformer processes the entire\n",
            "input sequence in parallel. Therefore, to provide the model with information about the order of\n",
            "tokens, positional encodings are added to the input embeddings [2].\n",
            "\n",
            "üìö SOURCES:\n",
            "- paper1.pdf (page 3)\n",
            "- paper1.pdf (page 2)\n",
            "- paper1.pdf (page 9)\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the research (type 'exit' to quit):\n",
            "Question 4: Describe the concept of multi-head attention in transformer architecture and why its beneficial\n",
            "\n",
            "üîç Searching and generating answer...\n",
            "\n",
            "==================================================\n",
            "üí° ANSWER:\n",
            "**Multi-Head Attention in Transformer Architecture**  Multi-head attention is a crucial component of\n",
            "the Transformer architecture, allowing the model to attend to information from different\n",
            "representation subspaces at different positions [3]. It addresses the limitation of single-head\n",
            "attention, where averaging can inhibit the model's ability to capture diverse relationships within\n",
            "the data [3].  The multi-head attention mechanism can be expressed as:  `MultiHead(Q, K, V) =\n",
            "Concat(head1, ..., headh)WO`  where `headi = Attention(QWQi, KWKi, VWVi)` [3].  Here: *   Q\n",
            "represents the queries. *   K represents the keys. *   V represents the values. *   WQ i ‚àà\n",
            "Rdmodel√ódk, WK i ‚àà Rdmodel√ódk, WV i ‚àà Rdmodel√ódv are the projection parameter matrices. *   WO ‚àà\n",
            "Rhdv√ódmodel is the projection matrix that transforms the concatenated outputs of the heads [3]. *\n",
            "h is the number of parallel attention heads [3].  In the described implementation, the Transformer\n",
            "employs h = 8 parallel attention layers (heads). For each head, dk = dv = dmodel/h = 64, where\n",
            "dmodel = 512 [3]. This design choice ensures that the total computational cost remains similar to\n",
            "single-head attention with full dimensionality due to the reduced dimension of each head [3].\n",
            "**Benefits of Multi-Head Attention**  1.  **Joint Attention from Different Representation\n",
            "Subspaces**: Multi-head attention enables the model to jointly attend to information from different\n",
            "representation subspaces [3]. This is beneficial because different attention heads can learn\n",
            "different relationships and dependencies within the data.  2.  **Counteracting Averaging Effects**:\n",
            "With a single attention head, averaging inhibits the model's ability to capture diverse\n",
            "relationships within the data [3]. Multi-head attention counteracts this by allowing each head to\n",
            "focus on different aspects of the input, preventing the averaging effect that can occur with a\n",
            "single attention mechanism [3].  3.  **Improved Performance**: By capturing a wider range of\n",
            "relationships and dependencies, multi-head attention contributes to improved model performance in\n",
            "various tasks, including translation quality [3].  In summary, multi-head attention enhances the\n",
            "Transformer's ability to model complex relationships within sequences by attending to different\n",
            "aspects of the data in parallel, leading to more expressive and powerful representations [3].\n",
            "\n",
            "üìö SOURCES:\n",
            "- paper1.pdf (page 5)\n",
            "- paper1.pdf (page 3)\n",
            "- paper1.pdf (page 2)\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the research (type 'exit' to quit):\n",
            "Question 5: What is few shot learning and how does GPT-3 implement it during inference\n",
            "\n",
            "üîç Searching and generating answer...\n",
            "\n",
            "==================================================\n",
            "üí° ANSWER:\n",
            "Few-shot learning is a method where a model is given a few demonstrations of a task at inference\n",
            "time as conditioning, but no weight updates are allowed [RWC+19]. GPT-3 implements few-shot learning\n",
            "by allowing as many demonstrations as will fit into the model‚Äôs context window (typically 10 to 100)\n",
            "[5]. The model is conditioned on these demonstrations during inference without updating its weights\n",
            "[RWC+19].\n",
            "\n",
            "üìö SOURCES:\n",
            "- paper2.pdf (page 34)\n",
            "- paper2.pdf (page 21)\n",
            "- paper2.pdf (page 6)\n",
            "- paper2.pdf (page 5)\n",
            "==================================================\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "DpTRzwmG9nB5"
      }
    }
  ]
}